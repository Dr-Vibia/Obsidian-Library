### 高斯消除

当我们系统性的学习了线性代数之后，我们可以把求解线性方程组的问题用**矩阵**来表达：
$$\begin{bmatrix}
3&4&1\\
4&2&6\\
1&1&1
\end{bmatrix}\cdot \begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}= \begin{bmatrix}
0\\
1\\
1
\end{bmatrix}$$
用矩阵的形式来表示之后，这个问题求解的方法也和之前的方法一样：我们可以把矩阵 $A$ 和 $b$ 的行与行之间相加或相减，然后最后得到未知数的结果。我们把这一类行与行之间操作求解未知数的方法统称为**高斯消除法（Gaussian Elimination）**。高斯消除问题系统性的定义就是，**给定一个矩阵 $A$ 和一个向量 $b$，能否找到一个向量 $x$，使得 $Ax=b$ 的关系满足。**

### 有噪音的高斯消除问题（Gaussian Elimination with Noise）

现在，我们把这个高斯消除问题变化一下，给它**增加一些难度**：增加噪音。

假如这个问题变成，如果已知一个矩阵 $A$，并且我们还知道一个向量:
$$\hat{b} =Ax+e$$
其中 $e$ 是我们在一个**固定数值范围内随机采集的一个随机噪音向量**，能否有效的通过 $A$ 和 $\hat{b}$ 的值来**还原最初的未知数向量 $x$ ？

加上这么一个随机噪音之后，我们线性代数的方法就已经不管用了，因为如果我们使用高斯消除法对每一行进行消除的时候，同时还会**带着噪音**进去，所以导致无法算出任何一个未知数的值。似乎唯一能够找到 $x$ 的方法就是**暴力破解**，一个一个猜 $x$ 的可能值，然后逐渐逼近 $\hat{b}$ 。

总结一下，带上了噪音之后，这个问题就变成了已知一个矩阵 $A$ ，和它与一个向量 $x$ 相乘得到的乘积再加上一定的**误差（error）**$e$，即 $Ax+e$，如何有效的**还原（learn）未知的向量。我们把这一类的问题统称为误差还原（Learning With Error， LWE）问题**。为了方便表述，我们后面都称之为 **LWE**。


### 搜索 LWE 问题（Search LWE Problem）的正式定义

- 首先，为了密码学的应用以及方便计算，我们使用 $Z_q$ 这么一个**素数有限域**。具体来说，也就是我们的世界里只存在 $(-q/2,q/2)$ 这个范围内的所有整数。
- 为了更好的**衡量噪音**，我们定义 $||e\in Z^m_q||_\infty$ 为向量 $e$（维度为 $m$）的**无限范数（Infinity Norm）**，具体操作就是看 $e$ 这个向量中的每一个值，然后返回最大的那个。$||e||_\infty=max^m_i|e_i|$
- 最后，我们定义 $x_B$ 就是为一个最大值封顶为 $B$ 的**随机分布**。也就是说，从这个随机分布中取出的每一个随机值都会小于 $B$ 。$\forall x\gets x^m_b:||x||_\infty \le B$

结合上述的概念，**搜索LWE问题的正式定义**如下：
$$
\begin{align*}
LWE(n,m,q,x_b):Search\ Version\\
Let\ A\overset{R}{\leftarrow}Z^{m\times n}_q,s\overset{R}{\leftarrow}Z_q,e\overset{R}{\leftarrow}x_B\\
Given(A,As+e),find\ s'\in Z^n_q\ s.t.\ ||As'-(As+e)||_\infty&\le B
\end{align*}
$$
简单的来说，在一个 LWE 问题当中，我们首先需要定义矩阵 $A$ 的**维度**为 $m\times n$ 。$m$ 代表了这个线性方程组**一共有多少组方程**，而 $n$ 代表了每个方程中有**多少个未知数**。我们还需要定义整个有限域 $Z_q$ 的大小 $q$，一般来说我们都会选择一个**足够大的素数**作为 $q$ 的值。最后，我们需要决定我们叠加的误差噪音的**取值上限** $B$ 。$B$ 的大小决定了我们 LWE 问题中需要找到的解距离实际的取值 $\hat{b}$ 究竟可以相差多少。

定义了上面的这些参数之后，LWE 问题就很好理解了：**给定矩阵 $A$ 以及带有误差的乘积 $As+e$，还原出未知的向量 $s$ 。**

我们可以再来仔细看一下每一个参数，试图理解一下**改变每个参数会怎么改变这个问题的难度**：

- $n$ 一般来说都被称为整个 LWE 问题的**安全参数（Security Parameter）**。如果一个系统中的未知变量越多（即 $n$ 越大），那么整个 LWE 问题会越难。
- $m$ 一般来说都是 $n$ 的一个**多项式倍数**，$m=poly(n)$。如果可以用的方程组越多（即 $m$ 越大），那么代表整个 LWE 问题会越简单。
- $q$ 一般也是 $n$ 的一个多项式倍数。一般来说，我们可以设置 $q$ 为 $O(n^2)$ 。
- 误差上限 $B$ 需要比 $q$ 小很多很多，$B<<q$ 。误差越小的话，那么找到正确的解相对来说越简单。

一般来说，这么多参数一个一个设置的话**太费劲**，所以我们都会只指定一个参数 $n$，然后把其他参数 $m,q,B$ 都设置成一个函数 $f(n)$ 的输出。只要参数设置的符合问题定义的要求，我们就可以保证随后生成的 LWE 问题实例（Problem instance）很大概率会拥有一个唯一的解 $x$ 。

### 从搜索 LWE（Search LWE）到决策 LWE（Decisional LWE）

决策 LWE（简称为**DLWE**）的设定和搜索 LWE（简称为**SLWE**）基本相同。唯一不同的是，SLWE 最后的问题是需要我们找到 $s$，而 DLWE 只需要让我们**辨别**看到的 $\hat{b}$ 到底是 LWE 问题中的**误差乘积**还是一个**随机生成的向量**。
$$
\begin{align*}
LWE(n,m,q,x_b):Decisional\ Version\\
Let\ A\overset{R}{\leftarrow}Z^{m\times n}_q,s\overset{R}{\leftarrow}Z_q,e\overset{R}{\leftarrow}x_B\\
Distinguish(A,As+e)\ from\ (A,v)
\end{align*}
$$
我们只能看到两个值，即 $A$ 和 $\hat{b}$，然后我们需要**辨别出**我们看到的到底是一个**LWE的问题实例**（即$\hat{b}=As+e$），还是一个**随机向量** $v$ 。在密码学中，**我们认为DLWE问题是困难的**，我们称之为**DLWE假设（DLWE Assumption）**。

为什么DLWE这个问题是困难的呢？其实道理很简单，**因为LWE问题本身就是困难的**，所以我们没有办法从$As+e$ 这么一个向量中提取出未知向量 $x$ 来。也就是说，在LWE问题中，在我们的视角里，$As+e$ 这个向量**和随机向量没有任何区别**，不会给我们提供任何有价值的信息。

这样一来，我们无法分辨出看到的向量 $\hat{b}$ 究竟是 LWE 中的 $As+e$ 还是一个随机的向量，正好符合 DLWE 假设。